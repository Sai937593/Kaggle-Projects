{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84896,"databundleVersionId":10305135,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ntrain_file_path = \"/kaggle/input/playground-series-s4e12/train.csv\"\ntest_file_path = \"/kaggle/input/playground-series-s4e12/test.csv\"\nsubmission_file_path = \"/kaggle/input/playground-series-s4e12/sample_submission.csv\"\n\ndf_train = pd.read_csv(train_file_path)\ndf_test = pd.read_csv(test_file_path)\ndf_submission = pd.read_csv(submission_file_path)","metadata":{"_uuid":"1ebb6d68-24ec-456d-8b46-f45bd1db17ec","_cell_guid":"8b1210a0-0abe-4c7c-ac30-7dbf2d48e24a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target = \"premium_amount\"\ndf_train.columns = [col.lower().replace(' ', '_') for col in df_train.columns]\ndf_test.columns = [col.lower().replace(' ', '_') for col in df_test.columns]\n\ndf_train = df_train.drop(columns=['id'])\ndf_test = df_test.drop(columns=['id'])\n\ncat_cols = [col for col in df_train.select_dtypes(include=['object']).columns if col != target]\nnum_cols = [col for col in df_train.select_dtypes(include=['int', 'float']).columns if col != target]\n\ncat_cols, num_cols","metadata":{"_uuid":"8e7b4f64-0c1a-4562-8d04-9cd9ca8b0053","_cell_guid":"38a2b264-cd59-4421-8090-6164e76aeda5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in cat_cols:\n    print(col, df_train[col].dtype, df_train[col].isna().sum())","metadata":{"_uuid":"208906f2-51a5-499b-8d80-a8d2f726144d","_cell_guid":"4e25453a-4108-4904-9839-01ce6f35d1b5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\ncat_imputer = SimpleImputer(strategy='most_frequent')\nnum_imputer = SimpleImputer(strategy='mean')\n\ndf_train_imputed = df_train.copy()\ndf_train_imputed[cat_cols] = cat_imputer.fit_transform(df_train[cat_cols], cat_cols)\ndf_train_imputed[num_cols] = num_imputer.fit_transform(df_train[num_cols], num_cols)\nprint(df_train_imputed.isna().sum())","metadata":{"_uuid":"3a7c8967-09b8-450f-b1c7-bd86a5736223","_cell_guid":"d6e9eae9-6f3e-48a9-9732-3d42fe5f6d37","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_cat_imputer = SimpleImputer(strategy='most_frequent')\ntest_num_imputer = SimpleImputer(strategy='mean')\ntest_cat_cols = df_test.select_dtypes(include=['object']).columns\ntest_num_cols = df_test.select_dtypes(exclude=['object']).columns\ndf_test_imputed = df_test.copy()\ndf_test_imputed[test_cat_cols] = test_cat_imputer.fit_transform(df_test[test_cat_cols])\ndf_test_imputed[test_num_cols] = test_num_imputer.fit_transform(df_test[test_num_cols])\nprint(df_test_imputed.isna().sum())\nfor col in num_cols:\n    if (df_train_imputed[col].nunique() <= 10):\n        df_train_imputed[col] = df_train_imputed[col].astype('int64')","metadata":{"_uuid":"1bb82c38-665b-42af-91a7-97ad71f2b1af","_cell_guid":"b3d62fae-0bcd-4cf5-9cda-eef105eb0e31","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntest_cat_imputer = SimpleImputer(strategy='most_frequent')\ntest_num_imputer = SimpleImputer(strategy='mean')\ntest_cat_cols = df_test.select_dtypes(include=['object']).columns\ntest_num_cols = df_test.select_dtypes(exclude=['object']).columns\ndf_test_imputed = df_test.copy()\ndf_test_imputed[test_cat_cols] = test_cat_imputer.fit_transform(df_test[test_cat_cols])\ndf_test_imputed[test_num_cols] = test_num_imputer.fit_transform(df_test[test_num_cols])\nprint(df_test_imputed.isna().sum())\nfor col in test_num_cols:\n    if (df_test_imputed[col].nunique() <= 10):\n        df_test_imputed[col] = df_test_imputed[col].astype('int64')","metadata":{"_uuid":"9f4a41f6-3d2d-4c9b-8122-49aec0673c9e","_cell_guid":"7617e9f9-9593-44fb-a8a2-fce396eb058a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nn_num_cols = len(num_cols)\nfig, axes = plt.subplots(n_num_cols // 3, 3, figsize=(12, 12))\n\nfor col, ax in zip(num_cols, axes.flatten()):\n    sns.boxplot(df_train_imputed[col], ax=ax)\n    ax.set_title(str(col))\nplt.show()","metadata":{"_uuid":"87fa92f4-0dca-474a-8ef4-b98fa2e4805b","_cell_guid":"06c3617e-50b0-41fb-9876-bc3a41f52f18","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor\n\nlof = LocalOutlierFactor(n_neighbors=35)\noutlier_labels = lof.fit_predict(df_train_imputed[num_cols])\ndf_train_imputed['outlier'] = outlier_labels\n\ndf_clean = df_train_imputed[df_train_imputed['outlier'] == 1]\nprint(df_clean.shape, df_train_imputed.shape)","metadata":{"_uuid":"817c36e6-c7fc-428d-821b-3a4929009ea2","_cell_guid":"b58df933-7599-4d64-b58f-871e01e49034","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nn_num_cols = len(num_cols)\nfig, axes = plt.subplots(n_num_cols // 3, 3, figsize=(12, 12))\n\nfor col, ax in zip(num_cols, axes.flatten()):\n    sns.boxplot(df_clean[col], ax=ax)\n    ax.set_title(str(col))\nplt.show()","metadata":{"_uuid":"e3d22a76-a87a-46ee-bd0f-d062b17dd0a8","_cell_guid":"5e6a7f8d-3990-474c-96f5-059872fffd99","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in num_cols:\n    print(col, df_train[col].nunique())","metadata":{"_uuid":"3f0d9918-0b1e-44bf-91f4-e8e06fd8b603","_cell_guid":"835d24b2-ceed-4fa0-b095-422a2ca68e8c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_imputed['insurance_duration']","metadata":{"_uuid":"15a850f2-6a83-43c9-966c-5b1ccf1d14c1","_cell_guid":"89c18632-db49-484c-addf-5fbe15159aa4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_train_test_cols(train_df, test_df):\n    test_cols = set(test_df.columns)\n    train_cols = set(train_df.columns)\n\n    return train_cols.difference(test_cols)","metadata":{"_uuid":"74ca423f-b52c-44b4-8832-75fbabf60c68","_cell_guid":"477336b3-6053-4ad4-bc8b-30c08c70d039","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_clean = df_clean.drop(columns=['policy_start_date'])\ndf_test_imputed = df_test_imputed.drop(columns=['policy_start_date'])","metadata":{"_uuid":"0872f0b3-1a49-49c8-a9ae-cb5e1b2a6ca5","_cell_guid":"9db5943a-1023-4420-824e-8489936edcc7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_clean = df_clean.copy()\ntrain_cat_cols = [col for col in cat_cols if col != 'policy_start_date']\ntrain_num_cols = num_cols\n\nfor col in train_cat_cols:\n    print(col, df_clean[col].unique())","metadata":{"_uuid":"7b5b959f-fbfe-4c24-862d-810e30f6c906","_cell_guid":"c5b0c91f-b9f7-4afa-8e64-fab8f0cfe4ee","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_encoded = pd.get_dummies(data=df_train_clean, columns=train_cat_cols)\ndf_test_encoded = pd.get_dummies(data=df_test_imputed, columns=[col for col in test_cat_cols if col!= 'policy_start_date'])\ndf_train_encoded = df_train_encoded.drop(columns=['outlier'])\ncheck_train_test_cols(df_train_encoded, df_test_encoded)","metadata":{"_uuid":"5d9d5429-3863-4b6f-a6be-ba21c62ef094","_cell_guid":"3efc555c-622e-4baf-8481-111d425523f7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_corr = df_train_encoded.corr()","metadata":{"_uuid":"84964d19-6d42-4595-96ae-58dbb71d497a","_cell_guid":"30ee1c6a-2e3b-416d-8079-1169780f083d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Set up the matplotlib figure\nplt.figure(figsize=(20, 20))  # Adjusted figure size\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(data=all_corr, vmin=-1.0, vmax=1.0, cmap='coolwarm', annot=True, annot_kws={\"size\": 8}, linewidths=.5)\n\n# Set the title\nplt.title('Correlation Matrix Heatmap', fontsize=20)\n\n# Display the heatmap\nplt.show()","metadata":{"_uuid":"2b552ae0-25a9-4196-b300-55775d66d7d3","_cell_guid":"5fc8dd3c-fc05-4eea-a2ec-b82e4fe818e4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_collinear_columns(corr_matrix, threshold):\n     \n    collinear_pairs = []\n    columns = corr_matrix.columns\n    \n    for i in range(len(columns)):\n        for j in range(i + 1, len(columns)):\n            if abs(corr_matrix.iloc[i, j]) > threshold:\n                col_pair = (columns[i], columns[j])\n                collinear_pairs.append(col_pair)\n    \n    return collinear_pairs\n\n\ncollinear_columns = find_collinear_columns(all_corr, 0.8)\nprint(\"Column pairs with collinearity exceeding threshold:\", collinear_columns)","metadata":{"_uuid":"82407bcf-5e01-4781-95e7-7185633956b6","_cell_guid":"0bd220c6-d987-4a60-86a4-3230d5e8649c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"collinear_cols_to_remove = [collinear_columns[i][0] for i in range(len(collinear_columns))]\ncollinear_cols_to_remove","metadata":{"_uuid":"3e9515a2-2348-4b01-9169-953d27d4a73e","_cell_guid":"38d4408c-207b-4914-a2d9-9757d6c58e2b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictor_cols = [col for col in df_train_encoded.columns if col!= target]\nX_corr_y = df_train_encoded[predictor_cols].corrwith(df_train_encoded[target])","metadata":{"_uuid":"eab0f8ba-e5cd-418c-aeaa-ab88012a0547","_cell_guid":"3772d752-fad0-42b8-84f1-960c3204c2f3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"abs(X_corr_y).sort_values(ascending=False)","metadata":{"_uuid":"ba2f4fc9-c4b7-45ae-b3f9-7526ba9a8d98","_cell_guid":"9be2e0cc-9fc3-47fb-8b6f-32af49ac3c82","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\n# Your existing code\nscaler = StandardScaler()\nX = df_train_encoded.drop(columns=target)\nX_scaled = scaler.fit_transform(X)\n\npca = PCA(n_components=0.95)\nX_reduced = pca.fit_transform(X_scaled)\n\n# Get feature importance scores\nfeature_importance = np.abs(pca.components_)\n\n# Get top features for each component\nn_features = X.shape[1]  # Number of original features\nselected_features = []\n\nfor i in range(pca.n_components_):\n    # Get indices of top features for this component\n    top_features_idx = np.argsort(feature_importance[i])[::-1]\n    # Get the corresponding feature names\n    top_features = X.columns[top_features_idx]\n    # Add to selected features\n    selected_features.extend(top_features)\n\n# Remove duplicates while preserving order\nselected_features = list(dict.fromkeys(selected_features))\n# Take only as many features as we have components\nselected_features = selected_features[:X_reduced.shape[1]]\n\n# Create DataFrame with selected original feature names\ndf_train_reduced = pd.DataFrame(X_reduced, columns=selected_features, index=X.index)\ndf_train_reduced[target] = df_train_encoded.loc[:, target]\nprint(df_train_reduced.shape, df_train_encoded.shape)\ndf_train_reduced.head(5)","metadata":{"_uuid":"c4550d6b-b6f9-4cf3-92b3-f3ea41329f2c","_cell_guid":"2bc40fa2-b88c-4c7c-872b-889fb42de67b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_reduced = df_test_encoded[[col for col in df_train_reduced.columns if col!= target]]\ndf_test_reduced_scaled = pd.DataFrame(scaler.fit_transform(df_test_reduced), columns=df_test_reduced.columns, index=df_test_reduced.index)\ncheck_train_test_cols(df_train_reduced, df_test_reduced_scaled)","metadata":{"_uuid":"07296df2-b583-41d8-8188-e53c06159385","_cell_guid":"03475a47-c17d-474a-94d4-413698163adc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_final = df_train_reduced.copy()\ndf_test_final = df_test_reduced_scaled.copy()\ndf_train_final.to_csv('df_train_preprocessed.csv')\ndf_test_final.to_csv('df_test_preprocessed.csv')","metadata":{"_uuid":"0d3b472e-eaf0-4770-b2b9-5b46b96b73df","_cell_guid":"3a5b75c3-e4c6-47c7-9828-70349b721f20","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_not_scaled = df_train_encoded[selected_features + [target]]\ndf_test_not_scaled = df_test_encoded[selected_features]\ncheck_train_test_cols(df_train_not_scaled, df_test_not_scaled)\ndf_train_not_scaled.to_csv('df_train.csv')\ndf_test_not_scaled.to_csv('df_test.csv')","metadata":{"_uuid":"73f7785a-55d5-4859-add2-b196b0db3426","_cell_guid":"c74eb1e1-cd7b-43c5-a259-65e8e09b013e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf_train = pd.read_csv('/kaggle/working/df_train.csv')\ndf_test = pd.read_csv('/kaggle/working/df_test.csv')\ndf_train.shape, df_test.shape","metadata":{"_uuid":"a2da2a87-494a-446b-be5b-924ea1e7d9d7","_cell_guid":"ff699091-6161-4f12-95f6-a1e2468872dd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-20T08:05:33.574240Z","iopub.execute_input":"2024-12-20T08:05:33.574600Z","iopub.status.idle":"2024-12-20T08:05:38.093293Z","shell.execute_reply.started":"2024-12-20T08:05:33.574566Z","shell.execute_reply":"2024-12-20T08:05:38.092264Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_top_n_imp_cols(corr_matrix, n):\n    c = abs(corr_matrix).sort_values(ascending=False)\n    top_n_cols = c[:n].index\n    return top_n_cols.tolist()\n\n\ndef prepare_train_validate(\n    X, y,\n    corr,\n    top_n_cols=-1,\n    target=\"premium_amount\",\n    norm=True,\n    scale=False,\n    \n):\n    \n    predictor_cols = get_top_n_imp_cols(corr, n=top_n_cols)\n    X = X.loc[:, predictor_cols]\n    if norm:\n        min_max_scaler = MinMaxScaler()\n        X = min_max_scaler.fit_transform(X=X)\n    if scale:\n        standard_scaler = StandardScaler()\n        X = standard_scaler.fit_transform(X=X)\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=0\n    )\n    return (X_train, X_val,y_train,  y_val)\n\n\n\ndef train_lin_reg(model, X_train, y_train, cv):\n    scores = cross_validate(estimator=model, X=X_train, y=y_train, scoring=scorer, cv=cv, n_jobs=-1, return_train_score=True)\n    mean_train_rmlse = np.mean(scores['train_score'])\n    mean_val_rmsle = np.mean(scores['test_score'])\n    train_mean_rmlse_scores.append(mean_train_rmlse)\n    val_mean_rmlse_scores.append(mean_val_rmsle)\n    print('rmlse_mean_train_scores', mean_train_rmlse)\n    print('rmlse_mean_validation_scores',mean_val_rmsle)","metadata":{"_uuid":"7c469e7a-daa5-47a7-b22d-56fce93f94c1","_cell_guid":"fcdc161a-0b96-4ff9-9782-adbaeb1ae9d1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-20T08:05:51.229381Z","iopub.execute_input":"2024-12-20T08:05:51.229997Z","iopub.status.idle":"2024-12-20T08:05:51.237643Z","shell.execute_reply.started":"2024-12-20T08:05:51.229960Z","shell.execute_reply":"2024-12-20T08:05:51.236796Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_mean_rmlse_scores = []\nval_mean_rmlse_scores = []\n\n\ntarget = 'premium_amount'\nX = df_train.drop(columns=target)\ny = df_train.loc[:, target]\ncorr = X.corrwith(y)\nn_top_cols_array = np.random.choice(np.arange(2, 27), size=10, replace=False)\nprint(n_top_cols_array)\nfor n_top_col in n_top_cols_array:\n    print('number of top cols = ', n_top_col)\n    X_train, X_test, y_train,  y_test = prepare_train_validate(X, y, corr, n_top_col)\n    train_lin_reg(lin_reg, X_train, y_train, cv=100)\n\nprint(train_mean_rmlse_scores, val_mean_rmlse_scores)","metadata":{"_uuid":"04798a4b-86c3-41af-9463-d6674278fdca","_cell_guid":"fb9d7682-ac16-4110-bb8b-1dc7e5e92208","trusted":true,"collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(n_top_cols_array, train_mean_rmlse_scores, label='Train RMSLE', marker='o')  # Plot train scores\nplt.plot(n_top_cols_array, val_mean_rmlse_scores, label='Validation RMSLE', marker='o')\nplt.legend()\nplt.show()","metadata":{"_uuid":"264dc7a3-b8ef-4e09-bf6c-b4e032a35cb5","_cell_guid":"0e750a68-4891-452d-9c9f-1adf24cdbdfc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.columns","metadata":{"_uuid":"7064ee95-5895-49b0-9bb4-e14f860a8487","_cell_guid":"e0837f8b-ebf5-4b26-9b22-94be67d68941","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ntarget='premium_amount'\nX = df_train.drop(columns=target)\ny = df_train.loc[:, target]\nprint(X.shape, y.shape)\n\ncorr = X.corrwith(y)\nX_train, X_test, y_train, y_test = prepare_train_validate(X, y, corr, top_n_cols=-1, scale=True)\n# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n# train_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n# val_dmatrix = xgb.DMatrix(data=X_val, label=y_val)\n\n#print(train_dmatrix.num_row(), train_dmatrix.num_col())\n#print(val_dmatrix.num_row(), val_dmatrix.num_col())\nprint(X_train.shape, X_test.shape)","metadata":{"_uuid":"4eb6cf19-88c3-47b1-98e5-b0340d93b456","_cell_guid":"87ec3d14-c7a5-471c-b233-0c1a174ad481","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-20T08:06:01.344194Z","iopub.execute_input":"2024-12-20T08:06:01.344959Z","iopub.status.idle":"2024-12-20T08:06:04.106252Z","shell.execute_reply.started":"2024-12-20T08:06:01.344923Z","shell.execute_reply":"2024-12-20T08:06:04.105330Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nimport optuna\n\n# Define RMSLE function\ndef rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\n# Custom evaluation function for RMSLE\ndef rmsle_eval(y_pred, dtrain):\n    y_true = dtrain.get_label()\n    score = rmsle(y_true, y_pred)\n    return 'rmsle', score\n\n# Objective function for Optuna\ndef objective(trial):\n    num_boost_round = trial.suggest_int(\"num_boost_round\", low=1700, high=1900, step=2)  # centered around 1750-1835\n    learning_rate = trial.suggest_float(\"learning_rate\", low=0.0205, high=0.04, log=True)  # centered around 0.0207-0.0210\n    max_depth = trial.suggest_int(\"max_depth\", low=2, high=8)  # centered around 5\n    reg_alpha = trial.suggest_float(\"reg_alpha\", low=7.70, high=7.85)  # centered around 7.73-7.81\n    reg_lambda = trial.suggest_float(\"reg_lambda\", low=5.35, high=5.42)  # centered around 5.37-5.41\n    subsample = trial.suggest_float(\"subsample\", low=0.8, high=0.9)  # centered around 0.810-0.811\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", low=0.7, high=0.9)  # centered around 0.790-0.796\n    gamma = trial.suggest_float(\"gamma\", low=4.60, high=4.72)  # centered around 4.63-4.71  # centered around 4.28-4.90\n    # Create the DMatrix for training\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n\n    # Set parameters for XGBoost\n    params = {\n        'objective': 'reg:squaredlogerror',  # Use squared log error objective\n        'eval_metric': 'rmsle',              # Track RMSLE\n        \n        'learning_rate': learning_rate,\n        'max_depth': max_depth,\n        'reg_alpha': reg_alpha,\n        'reg_lambda': reg_lambda,\n        'subsample': subsample,\n        'colsample_bytree': colsample_bytree,\n        'gamma': gamma,\n        'random_state': 42,\n        'tree_method': 'hist',  # Use 'hist' for faster training\n        'device': 'cuda',       # Use GPU\n    }\n\n    # Perform cross-validation\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        nfold=5,\n        early_stopping_rounds=10,\n        as_pandas=True,\n        seed=42,\n        custom_metric=rmsle_eval  # Use custom RMSLE evaluation function\n    )\n\n    # Return the best RMSLE from cross-validation\n    return cv_results['test-rmsle-mean'].min()\n\n\n# Create or load the study\nxgb_reg_study = optuna.create_study(\n    storage=\"sqlite:///xgb_reg_study.db\",\n    direction=\"minimize\",  # Minimize the RMSLE\n    load_if_exists=True,\n    study_name=\"xgb_reg_study\",\n)\n\n# Optimize the study\nxgb_reg_study.optimize(objective, n_trials=100, show_progress_bar=True)\n\n# Print the best parameters\nprint(\"Best parameters:\", xgb_reg_study.best_params)\nprint(\"Best validation score (RMSLE):\", xgb_reg_study.best_value)","metadata":{"_uuid":"fe6bedda-6908-4d3f-8907-5d0ec8b065e5","_cell_guid":"98622b8d-1f14-496f-8a08-8bc2782ee0e7","trusted":true,"collapsed":false,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-20T08:14:37.150807Z","iopub.execute_input":"2024-12-20T08:14:37.151161Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"[I 2024-12-20 08:14:37,190] Using an existing study with name 'xgb_reg_study' instead of creating a new one.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bc9635de34b43d1969d9ce2afcfa8d6"}},"metadata":{}},{"name":"stdout","text":"[I 2024-12-20 08:17:12,055] Trial 227 finished with value: 1.09733 and parameters: {'num_boost_round': 1846, 'learning_rate': 0.022069421526422394, 'max_depth': 4, 'reg_alpha': 7.807668719443181, 'reg_lambda': 5.373259758944974, 'subsample': 0.8175300294302966, 'colsample_bytree': 0.8312194865894137, 'gamma': 4.688020645182235}. Best is trial 176 with value: 1.0958708.\n[I 2024-12-20 08:19:49,591] Trial 228 finished with value: 1.097441194283634 and parameters: {'num_boost_round': 1842, 'learning_rate': 0.02149688831523411, 'max_depth': 4, 'reg_alpha': 7.805184350613749, 'reg_lambda': 5.3683064205396125, 'subsample': 0.8192169580070954, 'colsample_bytree': 0.8453456598543642, 'gamma': 4.680092943073093}. Best is trial 176 with value: 1.0958708.\n[I 2024-12-20 08:22:30,861] Trial 229 finished with value: 1.0958735855303048 and parameters: {'num_boost_round': 1834, 'learning_rate': 0.021116624585106793, 'max_depth': 4, 'reg_alpha': 7.81436384562176, 'reg_lambda': 5.371989843356312, 'subsample': 0.8131781273606092, 'colsample_bytree': 0.818055058106283, 'gamma': 4.684471084538716}. Best is trial 176 with value: 1.0958708.\n[I 2024-12-20 08:25:11,454] Trial 230 finished with value: 1.0958720000000002 and parameters: {'num_boost_round': 1838, 'learning_rate': 0.021272295919340652, 'max_depth': 4, 'reg_alpha': 7.810179451532859, 'reg_lambda': 5.374925179597544, 'subsample': 0.8156541442003752, 'colsample_bytree': 0.8112257211070365, 'gamma': 4.692039956926723}. Best is trial 176 with value: 1.0958708.\n[I 2024-12-20 08:27:47,120] Trial 231 finished with value: 1.0973368844334144 and parameters: {'num_boost_round': 1846, 'learning_rate': 0.021709023412859352, 'max_depth': 4, 'reg_alpha': 7.802608516829334, 'reg_lambda': 5.370035209337996, 'subsample': 0.817114210397619, 'colsample_bytree': 0.8159900784985856, 'gamma': 4.686534295998364}. Best is trial 176 with value: 1.0958708.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"print(\"Best parameters:\", xgb_reg_study.best_params)\nprint(\"Best validation score (RMSLE):\", xgb_reg_study.best_value)","metadata":{"_uuid":"504f3a81-20e0-4be7-bf49-c59e15938e4c","_cell_guid":"dd2bb136-8dda-4fe9-b5a4-85c012b62b0e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-20T07:01:50.311503Z","iopub.execute_input":"2024-12-20T07:01:50.311889Z","iopub.status.idle":"2024-12-20T07:01:50.330808Z","shell.execute_reply.started":"2024-12-20T07:01:50.311858Z","shell.execute_reply":"2024-12-20T07:01:50.329959Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert trials to DataFrame\ntrials_df = xgb_reg_study.trials_dataframe()\n\n# Print basic info about the study\nprint(f\"Total number of trials: {len(trials_df)}\")\nprint(f\"Best RMSLE achieved: {xgb_reg_study.best_value:.6f}\")\nprint(\"-\" * 50)\n\n# Get the best parameters directly from the study\nbest_params = xgb_reg_study.best_params\n\n# List of hyperparameters\nparams = [\n    'params_num_boost_round',\n    'params_learning_rate',\n    'params_max_depth',\n    'params_reg_alpha',\n    'params_reg_lambda',\n    'params_subsample',\n    'params_colsample_bytree',\n    'params_gamma'\n]\n\n# Calculate the threshold for top 10% of results\nn_best_trials = max(int(len(trials_df) * 0.1), 1)  # at least 1 trial\ntop_trials = trials_df.nsmallest(n_best_trials, 'value')\n\nprint(f\"\\nAnalyzing top {n_best_trials} trials:\")\nprint(f\"RMSLE range in top trials: {top_trials['value'].min():.6f} to {top_trials['value'].max():.6f}\")\nprint(\"-\" * 50)\n\n# Analyze each parameter\nfor param in params:\n    if param in trials_df.columns:\n        param_name = param.replace('params_', '')\n        print(f\"\\n{param_name}:\")\n        print(f\"Best value: {best_params[param_name]:.6f}\")\n        print(f\"Range in top trials: {top_trials[param].min():.6f} to {top_trials[param].max():.6f}\")\n        print(f\"Median in top trials: {top_trials[param].median():.6f}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Best Trial Configuration:\")\nprint(f\"RMSLE: {xgb_reg_study.best_value:.6f}\")\nfor param_name, value in best_params.items():\n    print(f\"{param_name}: {value:.6f}\")","metadata":{"_uuid":"fcef62bf-38bd-46cc-914f-321eeb6bdbdb","_cell_guid":"391fd228-ca16-4adc-bdea-a2c86dc1391f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the best parameters from the study\nbest_params = xgb_reg_study.best_params\n\n# Create final model parameters by combining best parameters with other required settings\nfinal_params = {\n    'objective': 'reg:squaredlogerror',\n    'eval_metric': 'rmsle',\n    'learning_rate': best_params['learning_rate'],\n    'max_depth': best_params['max_depth'],\n    'reg_alpha': best_params['reg_alpha'],\n    'reg_lambda': best_params['reg_lambda'],\n    'subsample': best_params['subsample'],\n    'colsample_bytree': best_params['colsample_bytree'],\n    'gamma': best_params['gamma'],\n    'random_state': 42,\n    'tree_method': 'hist',\n    'device': 'cuda'\n}\n\n# Create DMatrix for training and testing\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# Train the final model\nfinal_model = xgb.train(\n    final_params,\n    dtrain,\n    num_boost_round=best_params['num_boost_round'],\n    evals=[(dtrain, 'train'), (dtest, 'test')],\n    early_stopping_rounds=10,\n    verbose_eval=1000,  # Print evaluation every 10 rounds\n    custom_metric=rmsle_eval\n)\n\n# Make predictions\ntrain_predictions = final_model.predict(dtrain)\ntest_predictions = final_model.predict(dtest)\n\n# Calculate RMSLE on both training and test sets\ntrain_rmsle = rmsle(y_train, train_predictions)\ntest_rmsle = rmsle(y_test, test_predictions)\n\nprint(f\"Training RMSLE: {train_rmsle:.4f}\")\nprint(f\"Test RMSLE: {test_rmsle:.4f}\")\n\n\n\n\n# Calculate additional metrics if needed\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\nmae = mean_absolute_error(y_test, test_predictions)\nr2 = r2_score(y_test, test_predictions)\nrmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n\nprint(\"\\nAdditional Metrics:\")\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"R² Score: {r2:.4f}\")","metadata":{"_uuid":"d476cb21-4560-4c5b-a693-ba4e21eaae24","_cell_guid":"41ab89b2-07c4-4175-8567-81e014bad1e1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-20T07:01:58.904788Z","iopub.execute_input":"2024-12-20T07:01:58.905155Z","iopub.status.idle":"2024-12-20T07:02:36.149921Z","shell.execute_reply.started":"2024-12-20T07:01:58.905125Z","shell.execute_reply":"2024-12-20T07:02:36.149163Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nfrom keras import layers, initializers, regularizers, activations, Model, losses\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\n\n# Initialize MirroredStrategy\nstrategy = tf.distribute.MirroredStrategy()\nprint(f'Number of devices: {strategy.num_replicas_in_sync}')\n\nMODEL_PATH = 'best_ann_model.keras'\n\n# Custom LR Logger Callback\nclass LRLogger(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n        print(f'\\nCurrent learning rate: {lr:.6f}')\n\n# Check if model exists and get user choice\nif os.path.exists(MODEL_PATH):\n    while True:\n        choice = input(\"\\nSaved model found. Choose an option:\\n1: Load saved model\\n2: Resume training saved model\\n3: Train new model\\nYour choice (1, 2 or 3): \")\n        if choice in ['1', '2', '3']:\n            break\n        print(\"Invalid choice. Please enter 1, 2 or 3.\")\nelse:\n    print(\"\\nNo saved model found.\")\n    choice = '3'\n\nif choice == '1':\n    print(\"Loading saved model...\")\n    with strategy.scope():\n        ann_model = keras.models.load_model(MODEL_PATH)\n    print(\"Model loaded successfully!\")\n    \nelse:\n    # Common data preparation code for both resume training and new training\n    print(\"Preparing data...\")\n    y_train_pred = final_model.predict(dtrain)\n    train_residual_errors = abs(y_train - y_train_pred)\n\n    y_test_pred = final_model.predict(dtest)\n    test_residual_errors = abs(y_test - y_test_pred)\n\n    # Create train/validation split manually\n    X_train_main, X_val, residual_train, residual_val = train_test_split(\n        X_train, train_residual_errors, test_size=0.2, random_state=42\n    )\n\n    # Create datasets\n    GLOBAL_BATCH_SIZE = 512 * 2 * strategy.num_replicas_in_sync\n\n    # Training dataset\n    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_main, residual_train))\n    train_dataset = train_dataset.shuffle(1000).batch(GLOBAL_BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n\n    # Validation dataset\n    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, residual_val))\n    val_dataset = val_dataset.batch(GLOBAL_BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n\n    # Test dataset\n    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, test_residual_errors))\n    test_dataset = test_dataset.batch(GLOBAL_BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n\n    if choice == '2':\n        print(\"Loading saved model for continued training...\")\n        with strategy.scope():\n            ann_model = keras.models.load_model(MODEL_PATH)\n            \n            # Get current learning rate before asking\n            current_lr = float(tf.keras.backend.get_value(ann_model.optimizer.learning_rate))\n            \n            # Ask about learning rate\n            while True:\n                lr_choice = input(f\"\\nCurrent learning rate is: {current_lr:.6f}\\nChoose learning rate option:\\n1: Reset to new learning rate\\n2: Continue with current learning rate\\nYour choice (1 or 2): \")\n                if lr_choice in ['1', '2']:\n                    break\n                print(\"Invalid choice. Please enter 1 or 2.\")\n            \n            if lr_choice == '1':\n                # Ask for new learning rate\n                while True:\n                    try:\n                        new_lr = float(input(f\"Current learning rate is {current_lr:.6f}\\nEnter new learning rate (e.g., 0.001): \"))\n                        if new_lr > 0:\n                            break\n                        print(\"Learning rate must be positive.\")\n                    except ValueError:\n                        print(\"Please enter a valid number.\")\n                \n                # Reset learning rate to new value\n                ann_model.compile(\n                    optimizer=tf.keras.optimizers.Adam(learning_rate=new_lr),\n                    loss=losses.MeanSquaredLogarithmicError()\n                )\n                print(f\"Learning rate changed from {current_lr:.6f} to {new_lr:.6f}\")\n            else:\n                # Keep existing learning rate\n                ann_model.compile(\n                    optimizer=tf.keras.optimizers.Adam(learning_rate=current_lr),\n                    loss=losses.MeanSquaredLogarithmicError()\n                )\n                print(f\"Continuing with current learning rate: {current_lr:.6f}\")\n                \n        print(\"Model loaded successfully!\")\n        \n    else:  # choice == '3'\n        print(\"Creating new model...\")\n        if os.path.exists(MODEL_PATH):\n            confirm = input(\"Warning: Existing model will be overwritten. Continue? (y/n): \")\n            if confirm.lower() != 'y':\n                print(\"Training cancelled.\")\n                exit()\n        \n        # Ask for initial learning rate for new model\n        while True:\n            try:\n                initial_lr = float(input(\"Enter initial learning rate (e.g., 0.001): \"))\n                if initial_lr > 0:\n                    break\n                print(\"Learning rate must be positive.\")\n            except ValueError:\n                print(\"Please enter a valid number.\")\n                \n        # Create and compile model within strategy scope\n        with strategy.scope():\n            # Define model\n            input_layer = layers.Input(shape=(X_train.shape[1],))\n            x = input_layer\n            units = 520\n            for _ in range(5):\n                if units < 5:\n                    units = 5\n                x = layers.Dense(\n                    units,\n                    kernel_initializer=initializers.HeUniform,\n                    kernel_regularizer=regularizers.L1,\n                    activation=activations.gelu,\n                )(x)\n                x = layers.BatchNormalization()(x)\n                x = layers.Dropout(rate=0.7)(x)\n                units //= 2\n            output_layer = layers.Dense(1, activation=activations.linear)(x)\n\n            # Create model\n            ann_model = Model(inputs=input_layer, outputs=output_layer)\n            \n            # Compile model with MSLE and user-specified learning rate\n            ann_model.compile(\n                optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr),\n                loss=losses.MeanSquaredLogarithmicError()\n            )\n            print(f\"Model created with initial learning rate: {initial_lr:.6f}\")\n\n    # Add callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=35,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.25,\n            patience=3,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        tf.keras.callbacks.ModelCheckpoint(\n            MODEL_PATH,\n            monitor='val_loss',\n            save_best_only=True,\n            mode='min'\n        ),\n        tf.keras.callbacks.CSVLogger('training_log.csv', separator=',', append=True),\n        #LRLogger()\n    ]\n\n    # Get number of epochs for training\n    if choice == '2':\n        additional_epochs = int(input(\"Enter number of additional epochs for training: \"))\n        total_epochs = additional_epochs\n    else:\n        total_epochs = 50\n\n    # Train the model\n    print(f\"\\nStarting training for {total_epochs} epochs...\")\n    history = ann_model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=total_epochs,\n        callbacks=callbacks,\n        verbose=1\n    )\n\n    # Plot training history if matplotlib is available\n    try:\n        import matplotlib.pyplot as plt\n        \n        plt.figure(figsize=(12, 4))\n        \n        # Plot training loss\n        plt.subplot(1, 2, 1)\n        plt.plot(history.history['loss'], label='Training Loss')\n        plt.plot(history.history['val_loss'], label='Validation Loss')\n        plt.title('Model Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend()\n        \n        # Plot learning rate if available\n        if 'lr' in history.history:\n            plt.subplot(1, 2, 2)\n            plt.plot(history.history['lr'], label='Learning Rate')\n            plt.title('Learning Rate')\n            plt.xlabel('Epoch')\n            plt.ylabel('Learning Rate')\n            plt.yscale('log')\n            plt.legend()\n        \n        plt.tight_layout()\n        plt.show()\n    except Exception as e:\n        print(f\"Could not plot training history: {str(e)}\")\n\nprint(\"Model is ready for use!\")","metadata":{"_uuid":"bed80f99-1e3e-450c-9276-0b217596f14e","_cell_guid":"35f3b5c6-c04e-4297-95ff-bcb243efa82a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-20T07:04:06.313103Z","iopub.execute_input":"2024-12-20T07:04:06.313711Z","iopub.status.idle":"2024-12-20T07:04:30.452899Z","shell.execute_reply.started":"2024-12-20T07:04:06.313677Z","shell.execute_reply":"2024-12-20T07:04:30.451992Z"},"scrolled":true,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import mean_squared_log_error\nimport json\nimport gc\n\ndef rmsle(y_true, y_pred):\n    try:\n        return np.sqrt(mean_squared_log_error(y_true, y_pred))\n    except ValueError as e:\n        print(f\"Error in RMSLE calculation: {str(e)}\")\n        print(f\"Shapes: y_true: {y_true.shape}, y_pred: {y_pred.shape}\")\n        raise\n\ndef calculate_score_in_batches(y_true, base_pred, residuals, learning_rate, batch_size=1000):\n    total_squared_log_error = 0\n    total_samples = 0\n    \n    try:\n        # Ensure all inputs are numpy arrays and have correct shapes\n        y_true = np.array(y_true).reshape(-1)\n        base_pred = np.array(base_pred).reshape(-1)\n        residuals = np.array(residuals).reshape(-1)\n        \n        for i in range(0, len(y_true), batch_size):\n            # Get batch indices\n            end_idx = min(i + batch_size, len(y_true))\n            \n            # Get batch data\n            y_batch = y_true[i:end_idx]\n            base_pred_batch = base_pred[i:end_idx]\n            residuals_batch = residuals[i:end_idx]\n            \n            # Combine predictions for this batch\n            combined_pred_batch = base_pred_batch + learning_rate * residuals_batch\n            \n            # Calculate error for this batch\n            batch_score = rmsle(y_batch, combined_pred_batch)\n            total_squared_log_error += (batch_score ** 2) * len(y_batch)\n            total_samples += len(y_batch)\n            \n            # Clear batch variables\n            del y_batch, base_pred_batch, residuals_batch, combined_pred_batch\n            gc.collect()\n            \n    except Exception as e:\n        print(f\"Error in batch processing: {str(e)}\")\n        print(f\"Shapes: y_true: {y_true.shape}, base_pred: {base_pred.shape}, residuals: {residuals.shape}\")\n        return None\n    \n    return np.sqrt(total_squared_log_error / total_samples)\n\ntry:\n    # Define learning rates to test\n    learning_rates = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n    results = {}\n\n    # Get ANN predictions\n    print(\"Getting ANN predictions...\")\n    ann_train_residuals = ann_model.predict(X_train, batch_size=256)\n    ann_test_residuals = ann_model.predict(X_test, batch_size=256)\n\n    # Test each learning rate\n    best_test_score = float('inf')\n    best_lr = None\n\n    for lr in learning_rates:\n        print(f\"\\nTesting learning rate: {lr}\")\n        \n        print(\"Calculating training score...\")\n        train_score = calculate_score_in_batches(\n            y_train, \n            train_predictions, \n            ann_train_residuals, \n            lr,\n            batch_size=2500\n        )\n        \n        if train_score is None:\n            print(f\"Skipping learning rate {lr} due to training score calculation failure\")\n            continue\n\n        print(\"Calculating test score...\")\n        test_score = calculate_score_in_batches(\n            y_test, \n            test_predictions, \n            ann_test_residuals, \n            lr,\n            batch_size=2500\n        )\n        \n        if test_score is None:\n            print(f\"Skipping learning rate {lr} due to test score calculation failure\")\n            continue\n\n        # Calculate improvement\n        improvement = ((test_rmsle - test_score) / test_rmsle) * 100\n\n        print(f\"Learning Rate: {lr}\")\n        print(f\"Train RMSLE: {train_score:.6f}\")\n        print(f\"Test RMSLE: {test_score:.6f}\")\n        print(f\"Improvement: {improvement:.2f}%\")\n\n        # Store results\n        results[lr] = {\n            'train_rmsle': float(train_score),\n            'test_rmsle': float(test_score),\n            'improvement_percentage': float(improvement)\n        }\n\n        # Track best performing learning rate\n        if test_score < best_test_score:\n            best_test_score = test_score\n            best_lr = lr\n\n    # Print summary of results\n    print(\"\\nResults Summary:\")\n    print(\"-\" * 60)\n    print(f\"{'Learning Rate':^12} {'Train RMSLE':^15} {'Test RMSLE':^15} {'Improvement':^15}\")\n    print(\"-\" * 60)\n    for lr in learning_rates:\n        if lr in results:\n            print(f\"{lr:^12.2f} {results[lr]['train_rmsle']:^15.6f} {results[lr]['test_rmsle']:^15.6f} {results[lr]['improvement_percentage']:^15.2f}%\")\n    print(\"-\" * 60)\n    print(f\"\\nBest Learning Rate: {best_lr} (Test RMSLE: {best_test_score:.6f})\")\n\n    # Save the model and results\n    ann_model.save('ann_residual_model.keras')\n    model_info = {\n        'learning_rates_tested': learning_rates,\n        'best_learning_rate': best_lr,\n        'results': results,\n        'xgb_test_rmsle': float(test_rmsle)\n    }\n    with open('ann_model_info.json', 'w') as f:\n        json.dump(model_info, f, indent=4)\n    print(\"\\nModel and results saved successfully!\")\n\nexcept Exception as e:\n    print(f\"Error occurred: {str(e)}\")\n    print(\"Current shapes:\")\n    print(f\"y_train shape: {y_train.shape}\")\n    print(f\"train_predictions shape: {train_predictions.shape}\")\n    print(f\"ann_train_residuals shape: {ann_train_residuals.shape}\")\n    \nfinally:\n    # Clean up memory\n    print(\"\\nCleaning up memory...\")\n    gc.collect()","metadata":{"_uuid":"e63b0428-cfe3-4ddb-864f-7f0853437c61","_cell_guid":"b42e8cef-54da-4219-a327-a9ddd32c4be6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-20T07:04:59.402187Z","iopub.execute_input":"2024-12-20T07:04:59.402818Z","iopub.status.idle":"2024-12-20T07:16:51.592443Z","shell.execute_reply.started":"2024-12-20T07:04:59.402781Z","shell.execute_reply":"2024-12-20T07:16:51.591493Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/working/df_test.csv')\ndf_test = df_test.drop(columns=['Unnamed: 0'])\ndf_test.columns","metadata":{"_uuid":"4c36cea4-cd65-44e0-978b-5f057c317447","_cell_guid":"0cdac444-a978-4c1a-abf8-0a5e031927e7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_test_dmatrix = xgb.DMatrix(data=df_test)\ny_pred= final_model.predict(final_test_dmatrix)\ny_pred.shape, df_test.shape","metadata":{"_uuid":"ea355f65-2634-4fca-be69-4962c1051ca5","_cell_guid":"a5662f45-81e8-4930-a6c1-4d18640c6022","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_initial_final_test = pd.read_csv('/kaggle/input/playground-series-s4e12/test.csv')\nfinal_sub_df = pd.DataFrame(\n    {\n        'id': df_initial_final_test['id'],\n        'Premium Amount':y_pred\n    }\n)\nfinal_sub_df.tail(10)","metadata":{"_uuid":"1ff6fe35-823e-40ca-97ca-b1108408de04","_cell_guid":"7312fbcc-5cad-4549-89d6-303fed68d0bd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_sub_df.to_csv('submission_2.csv', index=False)","metadata":{"_uuid":"d133b52b-0904-490d-a4a9-9331f15608c2","_cell_guid":"bc165064-6ec1-4be0-9e68-47a80d2b103f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub_df = pd.read_csv('/kaggle/input/playground-series-s4e12/sample_submission.csv')\nsub_df.head(5)","metadata":{"_uuid":"5a8ab9f8-7f2d-4e1f-a2ac-d1cb32e4b7b8","_cell_guid":"76701a5b-039f-49fe-93b6-3dc6d02e9937","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert trials to DataFrame\ntrials_df = xgb_reg_study.trials_dataframe()\n\n# Print basic info about the study\nprint(f\"Total number of trials: {len(trials_df)}\")\nprint(f\"Best RMSLE achieved: {xgb_reg_study.best_value:.6f}\")\nprint(\"-\" * 50)\n\n# Get the best parameters directly from the study\nbest_params = xgb_reg_study.best_params\n\n# List of hyperparameters\nparams = [\n    'params_num_boost_round',\n    'params_learning_rate',\n    'params_max_depth',\n    'params_reg_alpha',\n    'params_reg_lambda',\n    'params_subsample',\n    'params_colsample_bytree',\n    'params_gamma'\n]\n\n# Calculate the threshold for top 10% of results\nn_best_trials = max(int(len(trials_df) * 0.1), 1)  # at least 1 trial\ntop_trials = trials_df.nsmallest(n_best_trials, 'value')\n\nprint(f\"\\nAnalyzing top {n_best_trials} trials:\")\nprint(f\"RMSLE range in top trials: {top_trials['value'].min():.6f} to {top_trials['value'].max():.6f}\")\nprint(\"-\" * 50)\n\n# Analyze each parameter\nfor param in params:\n    if param in trials_df.columns:\n        param_name = param.replace('params_', '')\n        print(f\"\\n{param_name}:\")\n        print(f\"Best value: {best_params[param_name]:.6f}\")\n        print(f\"Range in top trials: {top_trials[param].min():.6f} to {top_trials[param].max():.6f}\")\n        print(f\"Median in top trials: {top_trials[param].median():.6f}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Best Trial Configuration:\")\nprint(f\"RMSLE: {xgb_reg_study.best_value:.6f}\")\nfor param_name, value in best_params.items():\n    print(f\"{param_name}: {value:.6f}\")","metadata":{"_uuid":"d673766b-5a85-426c-bfb7-c9cf9a1e262c","_cell_guid":"9bf2a665-fcd7-46b3-8307-c357d1042d44","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}